from pathlib import Path
import random
from statistics import mean

import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn
from tqdm import tqdm

random_seed = 0
np.random.seed(random_seed)
torch.manual_seed(random_seed)
random.seed(random_seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

import Constants

n_way = 2
n_shot = 5
n_query = Constants.N_QUERY

DEVICE = "cuda"
n_workers = 1

# from easyfsl.datasets import CUB
from data.Covid.Covid import Covid
from easyfsl.samplers import TaskSampler
from torch.utils.data import DataLoader

n_tasks_per_epoch = 50
n_validation_tasks = 20

# Instantiate the datasets
train_set = Covid(split="train", training=True)
val_set = Covid(split="val", training=False)

# Those are special batch samplers that sample few-shot classification tasks with a pre-defined shape
train_sampler = TaskSampler(
    train_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_tasks_per_epoch
)
val_sampler = TaskSampler(
    val_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_validation_tasks
)

# Finally, the DataLoader. We customize the collate_fn so that batches are delivered
# in the shape: (support_images, support_labels, query_images, query_labels, class_ids)
train_loader = DataLoader(
    train_set,
    batch_sampler=train_sampler,
    collate_fn=train_sampler.episodic_collate_fn,
)
val_loader = DataLoader(
    val_set,
    batch_sampler=val_sampler,
    collate_fn=val_sampler.episodic_collate_fn,
)

n_test_tasks = 1000

test_set = Covid(split="test", training=False)
test_sampler = TaskSampler(
    test_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_test_tasks
)
test_loader = DataLoader(
    test_set,
    batch_sampler=test_sampler,
    collate_fn=test_sampler.episodic_collate_fn,
)

from easyfsl.methods import PrototypicalNetworks, FewShotClassifier
from easyfsl.modules import resnet12, resnet18
from torchvision.models import resnet18, vgg19_bn, ResNet18_Weights
from torchsummary import summary

rNet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
# rNet = resnet18()
rNet.fc = torch.nn.Identity()
requiresGrad = True
for param in rNet.parameters():
    param.requires_grad = requiresGrad
    requiresGrad = not requiresGrad
few_shot_classifier = PrototypicalNetworks(rNet).to(DEVICE)

# vNet = vgg19_bn(pretrained=True)
# vNet.classifier[6] = nn.Identity()
# few_shot_classifier = PrototypicalNetworks(vNet).to(DEVICE)

# convolutional_network = resnet12()
# few_shot_classifier = PrototypicalNetworks(convolutional_network).to(DEVICE)

from torch.optim import SGD, Optimizer
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.tensorboard import SummaryWriter

LOSS_FUNCTION = nn.CrossEntropyLoss()

n_epochs = 1200
scheduler_milestones = [200, 400, 600, 800, 1000]
scheduler_gamma = 0.2
learning_rate = 5e-3
tb_logs_dir = Path(".")

train_optimizer = SGD(
    few_shot_classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-3
)
# train_optimizer = torch.optim.Adam(few_shot_classifier.parameters(), lr=learning_rate, weight_decay=1e-2, momentum=0.9)

train_scheduler = MultiStepLR(
    train_optimizer,
    milestones=scheduler_milestones,
    gamma=scheduler_gamma,
)

tb_writer = SummaryWriter(log_dir=str(tb_logs_dir))


def training_epoch(
        model: FewShotClassifier, data_loader: DataLoader, optimizer: Optimizer
):
    all_loss = []
    model.train()
    with tqdm(
            enumerate(data_loader), total=len(data_loader), desc="Training"
    ) as tqdm_train:
        for episode_index, (
                support_images,
                support_labels,
                query_images,
                query_labels,
                _,
        ) in tqdm_train:
            torch.cuda.empty_cache()
            optimizer.zero_grad()
            model.process_support_set(
                support_images.to(DEVICE), support_labels.to(DEVICE)
            )
            classification_scores = model(query_images.to(DEVICE))

            loss = LOSS_FUNCTION(classification_scores, query_labels.to(DEVICE))
            loss.backward()
            optimizer.step()

            all_loss.append(loss.item())

            tqdm_train.set_postfix(loss=mean(all_loss))
            torch.cuda.empty_cache()

    return mean(all_loss)


from easyfsl.methods.utils import evaluate

# net_state_dict = "./data/Pascal/FewShotResNet18_999"
# few_shot_classifier.load_state_dict(torch.load(net_state_dict))

best_state = few_shot_classifier.state_dict()
best_validation_accuracy = 0.0
loss = []
valAccuracy = []
few_shot_classifier.use_softmax = True
for epoch in range(n_epochs):
    if epoch == 0:
        torch.save(few_shot_classifier.state_dict(), "./data/Covid/FewShotPretrainedResNet18Softmax_" + str(epoch))
    print(f"Epoch {epoch}")
    average_loss = training_epoch(few_shot_classifier, train_loader, train_optimizer)
    validation_accuracy = evaluate(
        few_shot_classifier, val_loader, device=DEVICE, tqdm_prefix="Validation"
    )
    loss.append(average_loss)
    valAccuracy.append(validation_accuracy)
    if validation_accuracy > best_validation_accuracy:
        best_validation_accuracy = validation_accuracy
        best_state = few_shot_classifier.state_dict()
        print("Ding ding ding! We found a new best model!")
        accuracy = evaluate(few_shot_classifier, test_loader, device=DEVICE)
        torch.save(few_shot_classifier.state_dict(), "./data/Covid/FewShotPretrainedResNet18Softmax_" + str(epoch))
        print(f"Average accuracy for current model: {(100 * accuracy):.2f} %")

    tb_writer.add_scalar("Train/loss", average_loss, epoch)
    tb_writer.add_scalar("Val/acc", validation_accuracy, epoch)

    # Warn the scheduler that we did an epoch
    # so it knows when to decrease the learning rate

    # if epoch % 50 ==49:
    #     pass

    if epoch % 200 == 199:
        accuracy = evaluate(few_shot_classifier, test_loader, device=DEVICE)
        torch.save(few_shot_classifier.state_dict(), "./data/Covid/FewShotPretrainedResNet18Softmax_" + str(epoch))
        print(f"Average accuracy for current model: {(100 * accuracy):.2f} %")
        print('Loss is')
        print(loss)
        print('ValAccuracy is')
        print(valAccuracy)
        # plt.plot(loss)
        # plt.show(block=True)

    train_scheduler.step()
    torch.cuda.empty_cache()

print('Loss is')
print(loss)
print('ValAccuracy is')
print(valAccuracy)
plt.plot(loss)
plt.show(block=True)




few_shot_classifier.load_state_dict(best_state)

torch.save(few_shot_classifier.state_dict(), "./data/Covid/FewShotPretrainedBestResnet18Softmax")

accuracy = evaluate(few_shot_classifier, test_loader, device=DEVICE)
print(f"Average accuracy for best model: {(100 * accuracy):.2f} %")


# Gen resnet 18, not pretrained
#Average accuracy for current model: 53.60 %
# Loss is
# [0.7006622385978699, 0.6794396662712097, 0.675051029920578, 0.6771990901231766, 0.6552673268318177, 0.6662291002273559, 0.647580851316452, 0.6477963882684707, 0.6539833015203476, 0.6602668392658234, 0.6599678337574005, 0.6691664952039719, 0.6498414993286132, 0.6455135852098465, 0.6747163999080658, 0.6294934946298599, 0.6579627203941345, 0.6458380073308945, 0.6348792600631714, 0.6220351564884186, 0.6216568171977996, 0.6419904273748398, 0.6312343353033065, 0.6448981642723084, 0.6400983732938766, 0.6057150036096572, 0.6373213535547256, 0.659828290939331, 0.667907042503357, 0.6203254044055939, 0.6372213119268417, 0.6410382908582687, 0.6026745402812957, 0.6579875665903091, 0.6702473205327988, 0.6262228149175644, 0.6427576369047165, 0.636494300365448, 0.6290977585315705, 0.6097177821397781, 0.6303092581033707, 0.6428068256378174, 0.6016767901182175, 0.6246415489912033, 0.6078919863700867, 0.6303647565841675, 0.605868456363678, 0.6057769012451172, 0.6497550594806671, 0.5903762131929398, 0.6183994400501251, 0.6217146110534668, 0.6062957209348678, 0.6400046670436859, 0.5858992797136306, 0.6266129863262176, 0.6233009445667267, 0.5967877668142318, 0.6030584061145783, 0.6180800276994706, 0.631374220252037, 0.6332168817520142, 0.6347423762083053, 0.6146946525573731, 0.6291836959123611, 0.5969293302297592, 0.6151009154319763, 0.623189502954483, 0.6295327347517014, 0.6202656835317611, 0.629268406033516, 0.6088028776645661, 0.6158345544338226, 0.5989469367265702, 0.6582570403814316, 0.5999098694324494, 0.5995030575990676, 0.6006213980913162, 0.6163549596071243, 0.6210017120838165, 0.6351479190587997, 0.6385172486305237, 0.6267056196928025, 0.6067145282030105, 0.6051565200090409, 0.5950609719753266, 0.6423373782634735, 0.6519242507219315, 0.6502635586261749, 0.625788659453392, 0.6277836918830871, 0.6513269460201263, 0.6440698689222336, 0.6468553155660629, 0.6173655724525452, 0.5986834049224854, 0.6270347452163696, 0.6169162195920944, 0.6457141458988189, 0.5846193665266037, 0.6331564342975616, 0.6205922710895538, 0.6182900261878967, 0.6027838337421417, 0.6059113937616348, 0.6318087232112884, 0.6286275964975357, 0.6045986694097519, 0.6160634261369705, 0.6344527530670167, 0.5881201636791229, 0.6246041035652161, 0.5984156167507172, 0.596290180683136, 0.6288398510217666, 0.6100945174694061, 0.5855990707874298, 0.5843210697174073, 0.6067983013391495, 0.6268515354394912, 0.6200405901670456, 0.5778472739458084, 0.6283856695890426, 0.6409025108814239, 0.5947355335950851, 0.6029331994056701, 0.619381383061409, 0.6346321004629135, 0.5917804366350174, 0.5839402371644974, 0.6312794810533524, 0.6128566908836365, 0.6141112744808197, 0.6100149881839753, 0.610348060131073, 0.6044873452186584, 0.6275642824172973, 0.6267217767238616, 0.6121833848953248, 0.6197620183229446, 0.6074554526805878, 0.6180119454860687, 0.5670886021852494, 0.6434601366519928, 0.6004216074943542, 0.6288215863704681, 0.6102411466836929, 0.585624692440033, 0.6208820748329162, 0.6368329501152039, 0.62349640250206, 0.6390695655345917, 0.6221032744646072, 0.6081474399566651, 0.5954019743204116, 0.6000141823291778, 0.6064436030387879, 0.638629577755928, 0.599534472823143, 0.6343034464120865, 0.6270782285928727, 0.6176817148923874, 0.6357047414779663, 0.6141814440488815, 0.5887276977300644, 0.5997221475839615, 0.6125185531377793, 0.6087653654813766, 0.6111196768283844, 0.6070169901847839, 0.6257524448633194, 0.6124662715196609, 0.6267783552408218, 0.6219219619035721, 0.6217705619335174, 0.6050072205066681, 0.5994769698381424, 0.6163698458671569, 0.6227582794427872, 0.6016826659440995, 0.6180792552232742, 0.6270960390567779, 0.6003353810310363, 0.5984397196769714, 0.5916008293628693, 0.6096665114164352, 0.6069726657867431, 0.6177903288602828, 0.5812336176633834, 0.5861275088787079, 0.6266331940889358, 0.6004340261220932, 0.6044828587770462, 0.6206591004133224, 0.6132015985250473, 0.6199099630117416, 0.6176791924238205, 0.6273041719198227, 0.6018566608428955, 0.6086950612068176, 0.6237745308876037, 0.5601695638895035, 0.5808770406246185, 0.6120779448747635, 0.5773818951845169, 0.6032205152511597, 0.595484271645546, 0.6133672106266022, 0.5816077172756196, 0.6106322014331818, 0.6105896580219269, 0.5706359088420868, 0.5863508975505829, 0.5990486067533493, 0.6061844098567962, 0.5923180299997329, 0.6199302393198013, 0.5808893197774887, 0.5958091276884079, 0.5825737565755844, 0.582718808054924, 0.5830842351913452, 0.5956266951560975, 0.5918140292167664, 0.5838033598661423, 0.5819326961040496, 0.5770441287755966, 0.5931235247850418, 0.5763099610805511, 0.6062305080890655, 0.5971106684207916, 0.5946927565336227, 0.5843499028682708, 0.5662786036729812, 0.6003551888465881, 0.6045971876382827, 0.5722156286239624, 0.5867490369081497, 0.6109541833400727, 0.5853349512815476, 0.5973465532064438, 0.609899223446846, 0.5778443658351898, 0.593053681254387, 0.5657023525238037, 0.5902286887168884, 0.5765362256765365, 0.5680935525894165, 0.5879046052694321, 0.5676740175485611, 0.5776952677965164, 0.6042324197292328, 0.5903679180145264, 0.6106139820814133, 0.5913927227258682, 0.5863345813751221, 0.6058000952005387, 0.5840114307403564, 0.5993875455856323, 0.6041432791948318, 0.6003261464834213, 0.5839417737722397, 0.5921146380901336, 0.5649898827075959, 0.5613427829742431, 0.5914381891489029, 0.5715772652626038, 0.5998844087123871, 0.6106903386116028, 0.5804257863759994, 0.5809580498933792, 0.5732740306854248, 0.5888502621650695, 0.5852899610996246, 0.6039330673217773, 0.5919205749034882, 0.5786364203691483, 0.5815892273187637, 0.5793710082769394, 0.5648597085475922, 0.5951715606451035, 0.6059880143404007, 0.5887751787900924, 0.6010139334201813, 0.6006190174818039, 0.5644263446331024, 0.5770799666643143, 0.605981211066246, 0.5815661466121673, 0.5917081439495087, 0.5987834614515305, 0.5774983948469162, 0.546901763677597, 0.5847029238939285, 0.5848673164844513, 0.5605151897668839, 0.5894899553060532, 0.5917117238044739, 0.586939834356308, 0.6102471578121186, 0.5790120029449463, 0.5380422997474671, 0.584679639339447, 0.6179868370294571, 0.5799646663665772, 0.5722367304563523, 0.5893144530057907, 0.5654463368654251, 0.5863090980052948, 0.582727066874504, 0.5823336887359619, 0.6054499804973602, 0.5959026324748993, 0.6034326630830765, 0.6067317199707031, 0.565488309264183, 0.5810981160402298, 0.5757782602310181, 0.6069136476516723, 0.5940965497493744, 0.5985025042295455, 0.5826985651254654, 0.5765058624744416, 0.5906155574321746, 0.6033762687444687, 0.5924747675657273, 0.5993276828527451, 0.6003910118341446, 0.5907998037338257, 0.6178740304708481, 0.5725785613059997, 0.5789929109811783, 0.5842853844165802, 0.5694379377365112, 0.599296823143959, 0.5835937595367432, 0.5896907079219819, 0.5915082222223282, 0.5738613659143448, 0.5786533445119858, 0.6010095697641372, 0.5787491250038147, 0.6155448681116105, 0.5751359683275222, 0.5829662299156189, 0.6104141891002655, 0.5932147216796875, 0.5848048907518387, 0.5780320197343827, 0.5804752212762833, 0.6116908478736878, 0.5954602724313736, 0.5965618807077407, 0.5937538254261017, 0.5707734376192093, 0.5628931218385697, 0.5779542261362076, 0.5756633377075195, 0.6087749582529068, 0.5827162021398544, 0.5761864972114563, 0.5882287245988845, 0.5690395188331604, 0.6060687345266342, 0.6002448242902756, 0.5805331468582153, 0.5885715293884277, 0.5897193688154221, 0.5624649935960769, 0.5654670655727386, 0.5776339954137802, 0.6036330020427704, 0.564970161318779, 0.559497846364975, 0.558804703950882, 0.5978430521488189, 0.598471046090126, 0.5739172112941742, 0.5579708588123321, 0.589225103855133, 0.5853016376495361, 0.5901087033748627, 0.597805860042572, 0.5878692871332168, 0.5667481207847596, 0.5950337231159211, 0.5824646824598312, 0.5644737231731415, 0.6016051453351975, 0.5925408577919007, 0.5737457871437073, 0.5603783637285232, 0.587116322517395, 0.5862764865159988, 0.5539979809522628, 0.5611405181884765, 0.5859929728507995, 0.5747574180364609, 0.5611150848865509, 0.6098673236370087, 0.5829250353574753, 0.5697466921806336, 0.6009256374835968, 0.5635117042064667, 0.5809482955932617, 0.5806958711147309, 0.5594876170158386, 0.5747498100996018, 0.5800083273649216, 0.5600496131181717, 0.5478327906131745, 0.5728446459770202, 0.5453606826066971, 0.565817899107933, 0.5541967380046845, 0.585207034945488, 0.5439486908912659, 0.5663831627368927, 0.5555228888988495, 0.5574176824092865, 0.5457049870491028, 0.5604450124502182, 0.5688748294115067, 0.5656767719984055, 0.571089962720871, 0.6033090609312057, 0.5800120782852173, 0.546329538822174, 0.5733560818433762, 0.619795314669609, 0.5682269990444183, 0.5877930617332459, 0.5514159387350083, 0.5760360413789749, 0.5597209912538529, 0.5613333547115326, 0.5719204390048981, 0.5608192616701126, 0.5517788356542588, 0.5780920433998108, 0.5842372113466263, 0.5927980810403823, 0.5683539688587189, 0.5546569108963013, 0.5539875006675721, 0.5329854053258896, 0.5447414618730545, 0.5652449023723602, 0.5709102571010589, 0.5568178874254227, 0.5465848332643509, 0.5945120626688003, 0.5679874712228775, 0.5631843686103821, 0.544023472070694, 0.5462506020069122, 0.568471913933754, 0.584972802400589, 0.5855806368589401, 0.5560497951507568, 0.5618509584665299, 0.5306920903921127, 0.5333070158958435, 0.5141961282491684, 0.5466594016551971, 0.5373846030235291, 0.5863460642099381, 0.5848304671049118, 0.5686916822195053, 0.5755326944589615, 0.5691236084699631, 0.5701995772123337, 0.5896067696809769, 0.5818782448768616, 0.5347214901447296, 0.5547141426801682, 0.5529629880189896, 0.5661685276031494, 0.5500569272041321, 0.54968132853508, 0.5485787999629974, 0.5652796167135239, 0.5443627578020096, 0.5647945672273635, 0.5379053711891174, 0.5666249829530716, 0.568516988158226, 0.548318681716919, 0.5587305843830108, 0.5405581831932068, 0.5358679342269898, 0.5350645619630814, 0.5634530860185624, 0.5504065245389939, 0.5612666124105453, 0.5566212981939316, 0.5482936799526215, 0.566507995724678, 0.5707073849439621, 0.5556095308065414, 0.5392511814832688, 0.5289113932847976, 0.5693516677618027, 0.5861511617898941, 0.5567960917949677, 0.5548141819238662, 0.5760315006971359, 0.5688852208852768, 0.5581325501203537, 0.5505837035179139, 0.6043252569437026, 0.5599799507856369, 0.5766096860170364, 0.5635862338542938, 0.5531100183725357, 0.5386874437332153, 0.5560454422235489, 0.5420401340723038, 0.5565052020549774, 0.5666121160984039, 0.5511180919408798, 0.5617204368114471, 0.5265192776918411, 0.5452074587345124, 0.5829851680994034, 0.5499551421403885, 0.5419273418188095, 0.5450603193044663, 0.5603372460603714, 0.5660240399837494, 0.6023840934038163, 0.5799677711725235, 0.5617849534749985, 0.5374918860197068, 0.5623052167892456, 0.5601519745588303, 0.5630952197313309, 0.5758092439174652, 0.5396523660421372, 0.5574304795265198, 0.5722187948226929, 0.5662566071748734, 0.5571145462989807, 0.5470575106143951, 0.5508381551504136, 0.5632342225313187, 0.5888528621196747, 0.5709474849700927, 0.5547193253040313, 0.580847544670105, 0.5815573781728745, 0.5418534326553345, 0.5199840313196182, 0.5518321198225021, 0.5456739717721939, 0.5593188822269439, 0.5810718703269958, 0.5654556411504745, 0.5602621239423752, 0.5429282289743423, 0.579808212518692, 0.5482498931884766, 0.5422327566146851, 0.5639461982250213, 0.5941184204816818, 0.5493178045749665, 0.5243615418672561, 0.5558649295568466, 0.557663648724556, 0.5581244117021561, 0.5438522690534592, 0.5577354747056961, 0.540838303565979, 0.5657728904485703, 0.5457203757762908, 0.571974778175354, 0.565057544708252, 0.5453758531808853, 0.550254847407341, 0.5564163970947266, 0.5623792850971222, 0.5544936692714691, 0.5615365439653397, 0.569656183719635, 0.5515658926963806, 0.5390579706430435, 0.5484112977981568, 0.5869796299934387, 0.56381352186203, 0.5466224718093872, 0.549642163515091, 0.5596371793746948, 0.5460076731443405, 0.5853582763671875, 0.572548177242279, 0.5334299862384796, 0.5648118370771408, 0.5436176973581314, 0.5320373916625977, 0.574539589881897, 0.5624848806858063, 0.5487012207508087, 0.5499590790271759, 0.5195430463552475, 0.555684564113617, 0.567782215476036, 0.5599637633562088, 0.5530096733570099, 0.5500814086198806, 0.5532524007558822, 0.5836738806962967, 0.5406397658586503, 0.52345967233181, 0.5718779301643372, 0.5452581018209457, 0.5491544502973557, 0.5397428065538407, 0.5663596618175507, 0.5449241214990616, 0.5543883311748504, 0.5338479787111282, 0.522711044549942, 0.5637482887506485, 0.5462159949541092, 0.5777115494012832, 0.5626227158308029, 0.5677392065525055, 0.5473223412036896, 0.5451672726869583, 0.5513836205005646, 0.5633020508289337, 0.5362763428688049, 0.557302206158638, 0.5578557902574539, 0.5875367546081542, 0.5401100647449494, 0.5648930406570435, 0.5588886344432831, 0.5441429513692856, 0.5611319774389267, 0.5089130264520645, 0.5852387470006942, 0.5578397876024246, 0.530823245048523, 0.5648712849617005, 0.5525301200151443, 0.5208572924137116, 0.5567620998620987, 0.5409759283065796, 0.5200600409507752, 0.548243036866188, 0.5631635719537735, 0.5471572595834732, 0.5449293726682662, 0.5338691926002502, 0.5513332152366638, 0.5306714314222336, 0.5580269473791123, 0.5375436615943908, 0.5663969343900681, 0.5459274226427078, 0.5362476766109466, 0.5450888228416443, 0.5237392330169678, 0.5564635545015335, 0.5299305558204651, 0.5608787608146667, 0.5532527339458465, 0.5404858130216599, 0.5360192531347274, 0.5571453976631164, 0.522584673166275, 0.5510770118236542, 0.5421617358922959, 0.5511116009950637, 0.5299989348649978, 0.5441153019666671, 0.5404725670814514, 0.5631451708078384, 0.5241479647159576, 0.5549346190690995, 0.5682102322578431, 0.5516112506389618, 0.5387982612848282, 0.5689401441812515, 0.5225389939546585, 0.5354263520240784, 0.5584721046686173, 0.528499675989151, 0.5216688394546509, 0.5564372199773788, 0.5523556768894196, 0.5189098286628723, 0.5286434537172318, 0.5484704500436783, 0.5407970327138901, 0.5598128175735474, 0.5220356237888336, 0.5738666105270386, 0.5203164160251618, 0.5279333525896073, 0.5532327765226364, 0.5593997770547867, 0.5638689273595809, 0.5201099181175232, 0.5544187504053116, 0.5172160619497299, 0.5554961752891541, 0.5568598175048828, 0.5412320625782013, 0.543270663022995, 0.5628017681837082, 0.5358583527803421, 0.5558053535223008, 0.5496864402294159, 0.5270868074893952, 0.5306675708293915, 0.5510680562257767, 0.525814071893692, 0.5701182335615158, 0.5592009019851685, 0.5351846414804459, 0.5518911790847778, 0.556930473446846, 0.5586142951250076, 0.517202005982399, 0.5211738806962967, 0.529051480293274, 0.5495051848888397, 0.5219786655902863, 0.5525913316011429, 0.5599966877698899, 0.5544623708724976, 0.5482217389345169, 0.5387549984455109, 0.5771740853786469, 0.5469410437345504, 0.549294645190239, 0.5462934082746506, 0.5605188709497452, 0.5300880473852158, 0.5426340478658677, 0.5395772498846054, 0.5747631335258484, 0.5608247518539429, 0.5397520458698273, 0.5581745600700379, 0.5329988664388656, 0.5296800810098649, 0.5603008908033371, 0.5488165479898452, 0.548888247013092, 0.5222349560260773, 0.5320608454942704, 0.5331971979141236, 0.5295063525438308, 0.5540525406599045, 0.5732881969213486, 0.5534242820739746, 0.5689684468507766, 0.5597573250532151, 0.5381672352552413, 0.5429462200403213, 0.5118038177490234, 0.5238081008195877, 0.5560866552591324, 0.547300894856453, 0.5360663175582886, 0.5579336965084076, 0.5452765417098999, 0.5453104555606842, 0.5273671996593475, 0.5417848181724548, 0.5353103214502335, 0.5224028253555297, 0.5645633244514465, 0.5400111049413681, 0.5418966442346573, 0.537647243142128, 0.5522194904088974, 0.5398916709423065, 0.534013187289238, 0.534198916554451, 0.5566540974378585, 0.5430432796478272, 0.5519601052999497, 0.5503849363327027, 0.5472970294952393, 0.5559913867712021, 0.562366943359375, 0.5519373536109924, 0.5605875355005264, 0.5378023988008499, 0.5350850719213486, 0.5634166049957275, 0.5146508836746215, 0.5598570853471756, 0.5458096885681152, 0.5101306849718094, 0.5386851561069489]
# ValAccuracy is
# [0.495, 0.53, 0.53, 0.5125, 0.53, 0.525, 0.525, 0.58, 0.46, 0.515, 0.495, 0.5375, 0.47, 0.4925, 0.5275, 0.5725, 0.5325, 0.54, 0.5275, 0.5025, 0.4825, 0.5175, 0.5, 0.56, 0.5925, 0.5275, 0.5475, 0.5825, 0.5675, 0.52, 0.515, 0.5125, 0.5025, 0.5125, 0.4675, 0.54, 0.5125, 0.5025, 0.535, 0.5075, 0.61, 0.54, 0.57, 0.555, 0.5025, 0.585, 0.5875, 0.525, 0.5175, 0.5525, 0.5, 0.5675, 0.495, 0.5375, 0.5375, 0.56, 0.5425, 0.5375, 0.535, 0.535, 0.5225, 0.52, 0.61, 0.5425, 0.52, 0.5675, 0.5525, 0.5325, 0.525, 0.5325, 0.55, 0.49, 0.5375, 0.56, 0.53, 0.5875, 0.535, 0.4875, 0.58, 0.53, 0.5025, 0.56, 0.54, 0.57, 0.545, 0.55, 0.535, 0.545, 0.5275, 0.52, 0.575, 0.5175, 0.53, 0.5625, 0.575, 0.555, 0.545, 0.5475, 0.4825, 0.52, 0.53, 0.545, 0.5225, 0.56, 0.5425, 0.555, 0.5225, 0.56, 0.5275, 0.5525, 0.545, 0.545, 0.5425, 0.5175, 0.485, 0.5825, 0.5225, 0.555, 0.59, 0.5425, 0.5325, 0.5325, 0.5275, 0.575, 0.48, 0.5575, 0.555, 0.5625, 0.57, 0.5575, 0.565, 0.585, 0.5475, 0.615, 0.5375, 0.5075, 0.5375, 0.475, 0.5175, 0.565, 0.5725, 0.565, 0.555, 0.57, 0.5075, 0.5775, 0.5375, 0.57, 0.4925, 0.4725, 0.565, 0.5375, 0.5825, 0.5375, 0.55, 0.5425, 0.5225, 0.5125, 0.525, 0.565, 0.585, 0.5125, 0.565, 0.5475, 0.565, 0.54, 0.535, 0.6075, 0.5875, 0.5275, 0.5625, 0.545, 0.47, 0.5525, 0.54, 0.5775, 0.4325, 0.54, 0.5525, 0.525, 0.485, 0.575, 0.555, 0.5225, 0.5425, 0.5925, 0.5925, 0.5625, 0.5675, 0.5175, 0.5, 0.5525, 0.505, 0.505, 0.525, 0.5175, 0.54, 0.5325, 0.53, 0.5275, 0.5925, 0.5475, 0.57, 0.53, 0.5275, 0.535, 0.5825, 0.5875, 0.575, 0.53, 0.51, 0.5875, 0.495, 0.5275, 0.5575, 0.53, 0.5625, 0.5375, 0.56, 0.56, 0.5575, 0.5925, 0.5025, 0.5475, 0.5725, 0.5175, 0.555, 0.56, 0.575, 0.55, 0.57, 0.59, 0.545, 0.5975, 0.5175, 0.58, 0.605, 0.5825, 0.5825, 0.58, 0.5675, 0.6225, 0.5575, 0.6375, 0.5775, 0.5825, 0.5675, 0.5375, 0.5675, 0.5925, 0.5375, 0.62, 0.49, 0.5825, 0.6025, 0.565, 0.565, 0.5825, 0.65, 0.53, 0.5575, 0.5825, 0.6125, 0.6425, 0.6, 0.6125, 0.575, 0.5375, 0.5425, 0.5175, 0.56, 0.5475, 0.5075, 0.5675, 0.53, 0.565, 0.555, 0.635, 0.585, 0.56, 0.565, 0.59, 0.5425, 0.495, 0.5875, 0.6125, 0.505, 0.585, 0.6, 0.4825, 0.56, 0.59, 0.6325, 0.505, 0.6325, 0.6125, 0.595, 0.57, 0.6325, 0.56, 0.62, 0.62, 0.54, 0.64, 0.6025, 0.6325, 0.6175, 0.53, 0.53, 0.6275, 0.6525, 0.6, 0.58, 0.5575, 0.61, 0.6325, 0.62, 0.6275, 0.5575, 0.54, 0.5675, 0.5725, 0.575, 0.6225, 0.6, 0.595, 0.6175, 0.6225, 0.6075, 0.535, 0.6025, 0.6075, 0.5675, 0.5425, 0.595, 0.595, 0.665, 0.5375, 0.615, 0.545, 0.6525, 0.5625, 0.61, 0.61, 0.5225, 0.5425, 0.5125, 0.595, 0.585, 0.5525, 0.615, 0.5725, 0.645, 0.5625, 0.5725, 0.5825, 0.535, 0.5875, 0.615, 0.5775, 0.5675, 0.5725, 0.5425, 0.6275, 0.61, 0.5325, 0.655, 0.69, 0.67, 0.635, 0.66, 0.6225, 0.5825, 0.6525, 0.56, 0.5875, 0.635, 0.65, 0.615, 0.625, 0.66, 0.6325, 0.585, 0.5725, 0.62, 0.6325, 0.6375, 0.67, 0.645, 0.6025, 0.68, 0.67, 0.605, 0.69, 0.525, 0.535, 0.645, 0.555, 0.6625, 0.625, 0.6225, 0.68, 0.6325, 0.6575, 0.7075, 0.615, 0.68, 0.6575, 0.6075, 0.61, 0.5975, 0.625, 0.625, 0.61, 0.6325, 0.655, 0.6325, 0.615, 0.5875, 0.615, 0.655, 0.5625, 0.6025, 0.6425, 0.635, 0.6575, 0.5575, 0.5675, 0.6425, 0.63, 0.6225, 0.6525, 0.635, 0.6025, 0.62, 0.615, 0.66, 0.6425, 0.5825, 0.585, 0.6475, 0.555, 0.6025, 0.615, 0.6525, 0.6975, 0.605, 0.665, 0.6725, 0.65, 0.6425, 0.635, 0.6075, 0.68, 0.605, 0.64, 0.665, 0.61, 0.6425, 0.635, 0.6725, 0.655, 0.6475, 0.64, 0.625, 0.65, 0.64, 0.69, 0.6425, 0.635, 0.655, 0.6275, 0.6725, 0.6975, 0.605, 0.64, 0.675, 0.6425, 0.65, 0.6025, 0.6225, 0.63, 0.655, 0.5625, 0.6575, 0.6175, 0.6075, 0.625, 0.625, 0.6, 0.6575, 0.625, 0.6825, 0.6775, 0.685, 0.69, 0.595, 0.6625, 0.6325, 0.6725, 0.7075, 0.67, 0.69, 0.66, 0.6075, 0.6, 0.615, 0.6275, 0.6425, 0.6825, 0.65, 0.625, 0.65, 0.6025, 0.6775, 0.6425, 0.6725, 0.675, 0.7075, 0.6125, 0.6475, 0.66, 0.645, 0.705, 0.665, 0.6725, 0.6575, 0.69, 0.6675, 0.66, 0.6125, 0.6475, 0.645, 0.6725, 0.6725, 0.5575, 0.62, 0.605, 0.6375, 0.675, 0.64, 0.705, 0.7, 0.67, 0.6175, 0.665, 0.6175, 0.6525, 0.65, 0.6675, 0.6275, 0.65, 0.6525, 0.6525, 0.6225, 0.68, 0.7075, 0.6875, 0.6925, 0.6325, 0.6425, 0.6475, 0.6725, 0.625, 0.63, 0.625, 0.6125, 0.615, 0.6025, 0.575, 0.6325, 0.6825, 0.68, 0.7025, 0.6075, 0.6275, 0.6625, 0.705, 0.72, 0.64, 0.6825, 0.7175, 0.6725, 0.6775, 0.67, 0.715, 0.6575, 0.6925, 0.6925, 0.64, 0.6175, 0.645, 0.67, 0.65, 0.645, 0.69, 0.6375, 0.6675, 0.66, 0.63, 0.675, 0.625, 0.6275, 0.6625, 0.6325, 0.7, 0.705, 0.655, 0.6175, 0.6375, 0.595, 0.655, 0.695, 0.6325, 0.68, 0.6025, 0.6575, 0.6725, 0.6525, 0.72, 0.6475, 0.675, 0.655, 0.7125, 0.72, 0.685, 0.6575, 0.6575, 0.725, 0.6425, 0.6375, 0.7075, 0.65, 0.6575, 0.6775, 0.645, 0.655, 0.62, 0.6875, 0.6575, 0.5975, 0.685, 0.63, 0.65, 0.6775, 0.665, 0.685, 0.675, 0.625, 0.67, 0.69, 0.6875, 0.705, 0.7175, 0.6675, 0.715, 0.685, 0.68, 0.7125, 0.7375, 0.6675, 0.7025, 0.7375, 0.5975, 0.64, 0.6325, 0.6675, 0.69, 0.6175, 0.6475, 0.62, 0.675, 0.6275, 0.6575, 0.6775, 0.665, 0.695, 0.6675, 0.73, 0.6775, 0.6625, 0.6375, 0.6725, 0.68, 0.7175, 0.7275, 0.695, 0.6825, 0.7125, 0.7175, 0.6325, 0.67, 0.6575, 0.6975, 0.68, 0.7225, 0.735, 0.69, 0.6875, 0.6775, 0.6425, 0.6625, 0.6925, 0.7225, 0.705, 0.7175, 0.6475, 0.6425, 0.6675, 0.655, 0.7275, 0.685, 0.66, 0.615, 0.72, 0.705, 0.66, 0.69, 0.675, 0.6675, 0.695, 0.6325, 0.6625, 0.6925, 0.7, 0.7225, 0.7125, 0.6975, 0.67, 0.665, 0.6975, 0.7125, 0.6675, 0.7175, 0.6425, 0.6625, 0.69, 0.7025, 0.675, 0.73, 0.735, 0.7125, 0.72, 0.7275, 0.7075, 0.705, 0.705, 0.715, 0.7075, 0.69, 0.6825, 0.685, 0.665, 0.665, 0.6825, 0.6575, 0.655, 0.68, 0.69, 0.73, 0.7025, 0.6875, 0.6925, 0.6725, 0.6775, 0.7525, 0.725, 0.6925, 0.7325, 0.7275, 0.66, 0.71, 0.6975, 0.675, 0.715, 0.71, 0.6975, 0.7125, 0.7025, 0.7175, 0.6425, 0.6325, 0.69, 0.67, 0.685, 0.7225, 0.69, 0.645, 0.6975, 0.6975, 0.7075, 0.7275, 0.6325, 0.685, 0.71, 0.74, 0.715, 0.73, 0.665, 0.695]
